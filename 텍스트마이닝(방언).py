# -*- coding: utf-8 -*-
"""텍스트마이닝(방언)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXk-vpOYtXRlm6RVZnAeqP8OaJbjWI1k
"""

import torch

from torch import nn

import torch.nn.functional as F

import torch.optim as optim

from torch.utils.data import Dataset, DataLoader

!pip install mxnet

!pip install gluonnlp tqdm
import gluonnlp as nlp

import numpy as np

from tqdm import tqdm, tqdm_notebook

!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master

!pip install sentencepiece
!pip install transformers

import os

from kobert.utils import get_tokenizer

from kobert.pytorch_kobert import get_pytorch_kobert_model

from transformers import AdamW

from transformers.optimization import get_cosine_schedule_with_warmup

device = torch.device("cuda:0")

bertmodel, vocab = get_pytorch_kobert_model()

import pandas as pd

from google.colab import files
uploaded = files.upload()

dataset_train = nlp.data.TSVDataset('train.tsv', field_indices = [0,1])

dataset_test = nlp.data.TSVDataset('test.tsv', field_indices = [0,1])

dataset_train[0]

dataset_test[0]

tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 2
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5

data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)

data_test = BERTDataset(dataset_test, 0,1, tok, max_len, True, False)

train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=2, shuffle=True)

test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=2, shuffle=True)

class BERTClassifier(nn.Module):
  def __init__(self, bert, hidden_size = 768, num_classes = 5, dr_rate = None, params = None):
    super(BERTClassifier, self).__init__()
    self.bert = bert
    self.dr_rate = dr_rate

    self.classifier = nn.Linear(hidden_size, num_classes)
    if dr_rate:
      self.dropout = nn.Dropout(p = dr_rate)

  def gen_attention_mask(self, token_ids, valid_length):
    attention_mask = torch.zeros_like(token_ids)
    for i, v in enumerate(valid_length):
      attention_mask[i][:v] = 1
    return attention_mask.float()

  def forward(self, token_ids, valid_length, segment_ids):
    attention_mask = self.gen_attention_mask(token_ids, valid_length)

    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
    if self.dr_rate:
      out = self.dropout(pooler)
      return self.classifier(out)

model = BERTClassifier(bertmodel, dr_rate = 0.5)

no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay':0.01},
                                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr = learning_rate)
loss_fn = nn.CrossEntropyLoss()

t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)

scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

def calc_accuracy(X,Y):
  max_vals, max_indices = torch.max(X, 1)
  train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
  return train_acc

###학습###

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long()
        segment_ids = segment_ids.long()
        valid_length= valid_length
        label = label.long()
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
          print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long()
        segment_ids = segment_ids.long()
        valid_length= valid_length
        label = label.long()
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))

!pip install torch

from torch._C import *

def softmax(vals, idx):
  valscpu = vals.cpu().detach().squeeze(0)
  a = 0
  for i in valscpu:
    a += np.exp(i)
  return ((np.exp(valscpu[idx]))/a).item() * 100

modelload = BERTClassifier(bertmodel,  dr_rate=0.5)

def testModel(model, seq):
  cate = ["강원도", "전라도", "제주도", "충청도", "경상도"]
  tmp = [seq]
  transform = nlp.data.BERTSentenceTransform(tok, max_len, pad = True, pair = False)
  tokenized = transform(tmp)

  modelload.eval()
  result = model(torch.tensor([tokenized[0]]), [tokenized[1]], torch.tensor(tokenized[2]))
  idx = result.argmax().cpu().item()
  print("위 방언은 : ", cate[idx], "지역 방언입니다.")
  print("신뢰도는 : ", "{:.2f}%".format(softmax(result, idx)))

testModel(model, "혼자 옵서")

testModel(model, "혼저 왕 먹읍서")

testModel(model, "죽었다 아임니꺼")

testModel(model, "가봤수과")

testModel(model, "해마두 집값이 오르는 걸까?")

testModel(model, "척하문 착이지!")

testModel(model, "한 번 해보는 건 어뗘?")

testModel(model, "진짜 하루를 알차게 사는 거여")

testModel(model, "너도 한 번 난중에 같이 해 볼려?")

testModel(model, "그래서 말 뜻이 뭐여")

testModel(model, "내한테 반했나? ")

testModel(model, "내한테 반했나? ")

testModel(model, "나는 검은색이나 흰색을 좋아하는 편인디")

##강원도##

testModel(model, "겨울 날쌔가 매우 차다.")

testModel(model, "괜히 헷수고 할 필요 없어.")

##전라도##

testModel(model, "인제 가자")

testModel(model, "내한테 반했나?")

##제주도##

testModel(model, "혼저 옵서예")

testModel(model, "저기 가봤수과")

##충청도##

testModel(model, "난중에 해봐야겠다.")

testModel(model, "시간을 허투루 보내고 있단 말여")

##경상도##

testModel(model, "어데 가?")

